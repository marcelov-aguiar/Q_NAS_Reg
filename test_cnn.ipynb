{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import os\n",
    "import torch\n",
    "#import tensorflow as tf\n",
    "from cnn.model import ConvBlock\n",
    "#import cnn_tf.model as tf_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable all tensorflow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "file_path = os.path.join(os.getcwd(), 'config_files', 'config2.txt')\n",
    "config_file = load_yaml(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with numpy an convert it to pytorch tensor\n",
    "X = np.random.rand(1, 6, 6, 3).astype(np.float32)\n",
    "x_torch = torch.from_numpy(X)\n",
    "#x_tf = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 3\n",
    "stride = 1\n",
    "filters = 32\n",
    "mu=0.9\n",
    "epsilon=2e-5\n",
    "conv_block = ConvBlock(kernel = kernel,\n",
    "                       in_channels = x_torch.shape[3],\n",
    "                       strides = stride, \n",
    "                       filters = filters, \n",
    "                       mu=mu,\n",
    "                       epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_conv_block = tf_model.ConvBlock(kernel = kernel,\n",
    "#                                    strides = stride,\n",
    "#                                    filters = filters,\n",
    "#                                    mu=mu, \n",
    "#                                    epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isclose(x_torch, x_tf).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_torch = conv_block(x_torch).detach().numpy()\n",
    "#y_tf = tf_conv_block(x_tf, name=\"conv_block\")\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isclose(y_torch, y_tf).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model import MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (1, 6, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "# create a a image tensor to test the max pooling layer\n",
    "max_pool = MaxPooling(kernel=3, strides=1)\n",
    "#tf_max_pool = tf_model.MaxPooling(kernel=3, strides=1)\n",
    "\n",
    "#y_tf = tf_max_pool(x_tf, name=\"max_pool\")\n",
    "y_torch = max_pool(x_torch).detach().numpy()\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isclose(y_torch, y_tf).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Avg Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (1, 6, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "from cnn.model import AvgPooling\n",
    "\n",
    "avg_pool = AvgPooling(kernel=3, strides=1)\n",
    "y_torch = avg_pool(x_torch).detach().numpy()\n",
    "\n",
    "#tf_avg_pool = tf_model.AvgPooling(kernel=3, strides=1)\n",
    "#y_tf = tf_avg_pool(x_tf, name='avg_pool')\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isclose(y_torch, y_tf).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stocastic Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (1, 6, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "from cnn.model import StochasticPooling\n",
    "\n",
    "stoc_pool = StochasticPooling(kernel=3, strides=1)\n",
    "y_torch = stoc_pool(x_torch).detach().numpy()\n",
    "\n",
    "#tf_stoc_pool = tf_model.AvgPooling(kernel=3, strides=1)\n",
    "#y_tf = tf_stoc_pool(x_tf, name='stoc_pool')\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 6, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (1, 6, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "from cnn.model import AttentionPooling\n",
    "\n",
    "stoc_pool = AttentionPooling(in_channels=x_torch.shape[1],kernel=3, strides=1)\n",
    "y_torch = stoc_pool(x_torch).detach().numpy()\n",
    "\n",
    "#tf_stoc_pool = tf_model.AvgPooling(kernel=3, strides=1)\n",
    "#y_tf = tf_stoc_pool(x_tf, name='stoc_pool')\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: (1, 6, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "from cnn.model import AttentionPooling2\n",
    "\n",
    "stoc_pool = AttentionPooling2(kernel=3, strides=1)\n",
    "y_torch = stoc_pool(x_torch).detach().numpy()\n",
    "\n",
    "#tf_stoc_pool = tf_model.AvgPooling(kernel=3, strides=1)\n",
    "#y_tf = tf_stoc_pool(x_tf, name='stoc_pool')\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with numpy an convert it to pytorch tensor\n",
    "X = np.random.rand(32, 6, 6, 3).astype(np.float32)\n",
    "x_torch = torch.from_numpy(X)\n",
    "#x_tf = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model import FullyConnected\n",
    "\n",
    "# flatten the tensor\n",
    "num_features = x_torch.shape[1] * x_torch.shape[2] * x_torch.shape[3]\n",
    "units = 20\n",
    "\n",
    "fc_torch = FullyConnected(input_features=num_features, units=units)\n",
    "#fc_tf = tf_model.FullyConnected(units=units)\n",
    "\n",
    "x_tor = torch.reshape(x_torch, [-1, num_features])\n",
    "#x_tfl = tf.reshape(x_tf, [-1, num_features])\n",
    "\n",
    "y_torch = fc_torch(x_tor).detach().numpy()\n",
    "#y_tf = fc_tf(x_tfl, name='fc')\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")\n",
    "print(f\"torch: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(y_torch, y_tf).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pad Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with numpy an convert it to pytorch tensor\n",
    "X = np.random.rand(1, 6, 6, 8).astype(np.float32)\n",
    "X1 = np.random.rand(1, 6, 6, 6).astype(np.float32)\n",
    "\n",
    "x_torch = torch.from_numpy(X)\n",
    "x_tf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "x_torch1 = torch.from_numpy(X1)\n",
    "x_tf1 = tf.convert_to_tensor(X1, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model import pad_features\n",
    "\n",
    "torch_tensors = [x_torch, x_torch1]\n",
    "tf_tensors = [x_tf, x_tf1]\n",
    "\n",
    "\n",
    "torch_padded = pad_features(tensors=torch_tensors)\n",
    "tf_padded = tf_model.pad_features(tensors=tf_tensors)\n",
    "\n",
    "torch_padded[0].shape, torch_padded[1].shape, tf_padded[0].shape, tf_padded[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ResidualV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with numpy an convert it to pytorch tensor\n",
    "X = np.random.rand(1, 3, 64, 64).astype(np.float32)\n",
    "x_torch = torch.from_numpy(X)\n",
    "#x_tf = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model import ResidualV1CBAM\n",
    "\n",
    "kernel = 3\n",
    "stride = 1\n",
    "filters = 32\n",
    "mu=0.9\n",
    "epsilon=2e-5\n",
    "Res_v1 = ResidualV1CBAM(kernel = kernel,\n",
    "                    in_channel = x_torch.shape[1],\n",
    "                    strides = stride, \n",
    "                    filters = filters, \n",
    "                    mu=mu,\n",
    "                    epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_Res = tf_model.ResidualV1(kernel = kernel,\n",
    "#                        strides = stride, \n",
    "#                        filters = filters, \n",
    "#                        mu=mu,\n",
    "#                        epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(x_torch, x_tf).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = Res_v1(x_torch).detach().numpy()\n",
    "#y_tf = tf_Res(x_tf, name=\"conv_block\")\n",
    "\n",
    "#print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(y_torch, y_tf).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ResidualV1Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with numpy an convert it to pytorch tensor\n",
    "X = np.random.rand(1, 6, 6, 3).astype(np.float32)\n",
    "x_torch = torch.from_numpy(X)\n",
    "x_tf = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.model import ResidualV1Pr\n",
    "\n",
    "kernel = 3\n",
    "stride = 1\n",
    "filters = 32\n",
    "mu=0.9\n",
    "epsilon=2e-5\n",
    "Res_v1Pr = ResidualV1Pr(kernel = kernel,\n",
    "                    in_channel = x_torch.shape[3],\n",
    "                    strides = stride, \n",
    "                    filters = filters, \n",
    "                    mu=mu,\n",
    "                    epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res_v1Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ResPR = tf_model.ResidualV1Pr(kernel = kernel,\n",
    "                       strides = stride, \n",
    "                       filters = filters, \n",
    "                       mu=mu,\n",
    "                       epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_torch = Res_v1Pr(x_torch).detach().numpy()\n",
    "y_tf = tf_ResPR(x_tf, name=\"conv_block\")\n",
    "\n",
    "print(f\"torch: {y_torch.shape}, tensorflow: {y_tf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Depthwise Separable Convolution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from cnn.model import DepthConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "in_channels = 3\n",
    "filters = 64\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthwise_conv = DepthConvBlock(kernel=kernel_size,\n",
    "                                in_channels=in_channels,\n",
    "                                filters=filters)\n",
    "input_data = torch.randn((1, in_channels, 64, 64))\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "\n",
    "output_data = depthwise_conv(input_data)\n",
    "print(\"Output shape:\", output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CBAM: Convolutional Block Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "in_channels = 3\n",
    "filters = 64\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deformable Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from cnn.model import DeformableConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "in_channels = 3\n",
    "filters = 64\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformable_block  = DeformableConvBlock(kernel=kernel_size,\n",
    "                                in_channels=in_channels,\n",
    "                                filters=filters)\n",
    "input_data = torch.randn((1, in_channels, 64, 64))\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "\n",
    "output_data = deformable_block(input_data)\n",
    "print(\"Output shape:\", output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformable_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inverted Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from cnn.model import InvertedResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "in_channels = 3\n",
    "filters = 64\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_block  = InvertedResidualBlock(kernel=kernel_size,\n",
    "                                in_channels=in_channels,\n",
    "                                filters=filters)\n",
    "input_data = torch.randn((1, in_channels, 64, 64))\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "\n",
    "output_data = inverted_block(input_data)\n",
    "print(\"Output shape:\", output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedResidualBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Flops Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fvcore.nn import FlopCountAnalysis # https://github.com/facebookresearch/fvcore/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "input = torch.randn(10, 3, 32, 32)\n",
    "\n",
    "# Compute FLOPS\n",
    "flop_counter = FlopCountAnalysis(model, input)\n",
    "print(f\"FLOPs: {flop_counter.total()}\")\n",
    "\n",
    "# Alternatively, print a detailed breakdown\n",
    "print(flop_counter.by_operator())\n",
    "\n",
    "# total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_counter.by_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NetworkGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import qnas_config as cfg\n",
    "from cnn.model import NetworkGraph\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'evolution'\n",
    "experiment_path = 'my_exp_config3'\n",
    "config_file = 'config_files_cifar/config14.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'experiment_path': experiment_path,\n",
    "    'config_file': config_file,\n",
    "    'data_path': 'cifar10_data',\n",
    "    'log_level': 'INFO',\n",
    "    'log_level': 'INFO',\n",
    "    'fitness_metric': 'best_accuracy',\n",
    "    'optimizer': 'adamw',\n",
    "    'data_augmentation': 'False',\n",
    "    'dataset': 'ciaf10',\n",
    "    'save_checkpoints_epochs': 10,\n",
    "    'early_stopping': 'False',\n",
    "    'en_pop_crossover': 'False',\n",
    "    'network_config': 'default',\n",
    "    'network_gap': 'False',\n",
    "    'limit_data_value': 100000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cfg.ConfigParameters(args, phase=phase)\n",
    "config.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_dict=config.fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_list = ['conv_3_1_64', 'avg_pool_2_2', 'avg_pool_2_2', 'conv_5_1_64', 'conv_5_1_128', 'conv_5_1_64']\n",
    "#net_list = ['max_pool_2_2', 'no_op', 'no_op', 'conv_5_1_64', 'no_op', 'no_op']\n",
    "net_list = ['seconv_1_1_32_red4', 'seconv_1_1_32', 'max_pool_2_2', 'conv_1_1_32', 'max_pool_2_2', 'max_pool_2_2', 'conv_1_1_64', \n",
    "            'avg_pool_2_2', 'mbconv_3_1_32', 'conv_3_1_64', 'avg_pool_2_2', 'no_op', 'avg_pool_2_2', 'no_op', \n",
    "            'mbeppga_3_1_128', 'no_op', 'mbeppga_3_1_32_expand3_red16', 'mbconv_3_1_32', 'no_op', 'no_op']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_1_1_32': {'function': 'ConvBlock',\n",
       "  'params': {'kernel': 1, 'strides': 1, 'filters': 32}},\n",
       " 'conv_1_1_64': {'function': 'ConvBlock',\n",
       "  'params': {'kernel': 1, 'strides': 1, 'filters': 64}},\n",
       " 'conv_3_1_64': {'function': 'ConvBlock',\n",
       "  'params': {'kernel': 3, 'strides': 1, 'filters': 64}},\n",
       " 'seconv_1_1_32': {'function': 'SEConvBlock',\n",
       "  'params': {'kernel': 1, 'strides': 1, 'filters': 32, 'reduction_ratio': 16}},\n",
       " 'seconv_1_1_32_red4': {'function': 'SEConvBlock',\n",
       "  'params': {'kernel': 1, 'strides': 1, 'filters': 32, 'reduction_ratio': 4}},\n",
       " 'mbconv_3_1_32': {'function': 'MBConv',\n",
       "  'params': {'kernel': 3,\n",
       "   'strides': 1,\n",
       "   'filters': 32,\n",
       "   'expand_ratio': 6,\n",
       "   'reduction_ratio': 16}},\n",
       " 'mbeppga_3_1_128': {'function': 'MBConv_EPPGA',\n",
       "  'params': {'kernel': 3,\n",
       "   'strides': 1,\n",
       "   'filters': 128,\n",
       "   'expand_ratio': 6,\n",
       "   'reduction_ratio': 16}},\n",
       " 'mbeppga_3_1_32_expand3_red16': {'function': 'MBConv_EPPGA',\n",
       "  'params': {'kernel': 3,\n",
       "   'strides': 1,\n",
       "   'filters': 32,\n",
       "   'expand_ratio': 3,\n",
       "   'reduction_ratio': 16}},\n",
       " 'max_pool_2_2': {'function': 'MaxPooling',\n",
       "  'params': {'kernel': 2, 'strides': 2}},\n",
       " 'avg_pool_2_2': {'function': 'AvgPooling',\n",
       "  'params': {'kernel': 2, 'strides': 2}},\n",
       " 'no_op': {'function': 'NoOp', 'params': {}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dict = {key: item for key, item in fn_dict.items() if key in net_list}\n",
    "filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NetworkGraph(num_classes=10, in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network_config: default\n",
      "expand_ratio: 6\n",
      "mid_channels: 16\n",
      "expand_ratio: 6\n",
      "mid_channels: 16\n",
      "expand_ratio: 3\n",
      "mid_channels: 16\n",
      "expand_ratio: 6\n",
      "mid_channels: 16\n"
     ]
    }
   ],
   "source": [
    "net.create_functions(fn_dict=filtered_dict, net_list=net_list)\n",
    "input_random = torch.randn(256, 3, 32, 32)\n",
    "\n",
    "_ = net(input_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkGraph(\n",
       "  (model): Sequential(\n",
       "    (0): MaxPooling(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (activation): ReLU()\n",
       "      (conv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): MaxPooling(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): MaxPooling(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (activation): ReLU()\n",
       "      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): AvgPooling(\n",
       "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (6): MBConv(\n",
       "      (expand_conv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (expand_relu): ReLU6(inplace=True)\n",
       "      (depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "      (depthwise_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (depthwise_relu): ReLU6(inplace=True)\n",
       "      (se_block): SEBlock(\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (project_conv): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): ConvBlock(\n",
       "      (activation): ReLU()\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): AvgPooling(\n",
       "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (9): AvgPooling(\n",
       "      (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (10): MBConv_EPPGA(\n",
       "      (expand_conv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (expand_relu): ReLU6(inplace=True)\n",
       "      (depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "      (depthwise_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (depthwise_relu): ReLU6(inplace=True)\n",
       "      (pointwise_conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pointwise_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se_block): SEBlock(\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (11): MBConv_EPPGA(\n",
       "      (expand_conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (expand_relu): ReLU6(inplace=True)\n",
       "      (depthwise_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "      (depthwise_bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (depthwise_relu): ReLU6(inplace=True)\n",
       "      (pointwise_conv): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pointwise_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (se_block): SEBlock(\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (12): MBConv(\n",
       "      (expand_conv): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (expand_relu): ReLU6(inplace=True)\n",
       "      (depthwise_conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "      (depthwise_bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (depthwise_relu): ReLU6(inplace=True)\n",
       "      (se_block): SEBlock(\n",
       "        (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (fc2): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (project_conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): FullyConnected(\n",
       "    (fc): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2, 3, 32, 32).astype(np.float32)\n",
    "x_torch = torch.from_numpy(X)\n",
    "#x_tf = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = net(inputs=x_torch, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.input import GenericDataLoader\n",
    "import copy\n",
    "import qnas_config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'evolution'\n",
    "experiment_path = 'my_exp_config3'\n",
    "config_file = 'config_files_cifar/config2.txt'\n",
    "\n",
    "args = {\n",
    "    'experiment_path': experiment_path,\n",
    "    'config_file': config_file,\n",
    "    'data_path': 'atleta_coronal_data',\n",
    "    'log_level': 'INFO',\n",
    "    'fitness_metric': 'best_accuracy',\n",
    "    'optimizer': 'adamw',\n",
    "    'data_augmentation': 'False',\n",
    "    'dataset': 'atleta_coronal',\n",
    "    'save_checkpoints_epochs': 10,\n",
    "    'early_stopping': 'False',\n",
    "}\n",
    "\n",
    "config = cfg.ConfigParameters(args, phase=phase)\n",
    "config.get_parameters()\n",
    "\n",
    "fn_dict=config.fn_dict\n",
    "fn_dict_tf = copy.deepcopy(fn_dict)\n",
    "fn_dict\n",
    "\n",
    "net_list = ['conv_3_1_256','no_op', 'conv_3_1_128', 'no_op','no_op','no_op',\n",
    "            'conv_3_1_64','conv_3_1_64','no_op','conv_3_1_256','conv_3_1_256',\n",
    "            'max_pool_2_2', 'conv_3_1_128', 'no_op','no_op','no_op','no_op','no_op',\n",
    "            'max_pool_2_2', 'conv_3_1_128']\n",
    "\n",
    "params = config.train_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GenericDataLoader(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = data_loader.get_loader(pin_memory_device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_images = next(iter(val_loader))[0][:10]\n",
    "selected_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the channels of the first image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    axs[i].imshow(selected_images[0, i, :, :].detach().numpy(), cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(val_loader))\n",
    "min_value = data[0].min().item()\n",
    "max_value = data[0].max().item()\n",
    "\n",
    "print(\"Min Value:\", min_value)\n",
    "print(\"Max Value:\", max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of train dataset: {len(train_loader)}\")\n",
    "print(f\"Size of validation dataset: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to count images per class\n",
    "class_counts = {}  # Create an empty dictionary\n",
    "\n",
    "# Iterate through the DataLoader to count images per class\n",
    "loaders = [train_loader, val_loader]\n",
    "for loader in loaders:\n",
    "    for batch in loader:\n",
    "        _, targets = batch\n",
    "        for label in targets:\n",
    "            label = label.item()  # Convert the tensor label to an integer\n",
    "            if label not in class_counts:\n",
    "                class_counts[label] = 0  # Initialize the count to 0 if it doesn't exist\n",
    "            class_counts[label] += 1\n",
    "    print(\"-\" * 50)\n",
    "    # Print the counts for each class\n",
    "    for class_label, count in class_counts.items():\n",
    "        print(f\"Class {class_label}: {count} images\")\n",
    "    class_counts = {}  # Reset the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "unorm = UnNormalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some images of the pytorch train_loader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship','truck']\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "cols, rows = 5, 3\n",
    "train_imgs, train_labels = next(iter(train_loader))\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_imgs), size=(1,)).item()\n",
    "    img, label = train_imgs[sample_idx], train_labels[sample_idx]\n",
    "    img = unorm(img)\n",
    "    npimg = img.numpy()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(cifar10_classes[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.autoscale(enable=True, axis='both', tight=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'evolution'\n",
    "experiment_path = 'my_exp_config3'\n",
    "config_file = 'config_files_cifar/config2.txt'\n",
    "\n",
    "args = {\n",
    "    'experiment_path': experiment_path,\n",
    "    'config_file': config_file,\n",
    "    'data_path': 'cifar100_data',\n",
    "    'log_level': 'INFO',\n",
    "    'fitness_metric': 'best_accuracy',\n",
    "    'optimizer': 'adamw',\n",
    "    'data_augmentation': 'False',\n",
    "    'dataset': 'Cifar10',\n",
    "    'save_checkpoints_epochs': 10,\n",
    "}\n",
    "\n",
    "config = cfg.ConfigParameters(args, phase=phase)\n",
    "config.get_parameters()\n",
    "\n",
    "fn_dict=config.fn_dict\n",
    "fn_dict_tf = copy.deepcopy(fn_dict)\n",
    "fn_dict\n",
    "\n",
    "net_list = ['conv_3_1_256','no_op', 'conv_3_1_128', 'no_op','no_op','no_op',\n",
    "            'conv_3_1_64','conv_3_1_64','no_op','conv_3_1_256','conv_3_1_256',\n",
    "            'max_pool_2_2', 'conv_3_1_128', 'no_op','no_op','no_op','no_op','no_op',\n",
    "            'max_pool_2_2', 'conv_3_1_128']\n",
    "\n",
    "params = config.train_spec\n",
    "params['dataset'] = \"Cifar100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_cifar100 = GenericDataLoader(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = data_loader_cifar100.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_loader: {len(train_loader)}, val_loader: {len(val_loader)}, test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unorm_ = UnNormalize(mean=(0.5071, 0.4865, 0.4411), std=(0.2673, 0.2564, 0.2761))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(5, 5))\n",
    "cols, rows = 3, 3\n",
    "train_imgs, train_labels = next(iter(train_loader))\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_imgs), size=(1,)).item()\n",
    "    img, label = train_imgs[sample_idx], train_labels[sample_idx]\n",
    "    img = unorm_(img)\n",
    "    npimg = img.numpy()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import calculate_time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "step_time = 60 * 10 # 10 minutes\n",
    "end = 5 * step_time # 50 minutes\n",
    "calculate_time(start, end, 5, 20, end_evol=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import qnas_config as cfg\n",
    "from cnn.input import GenericDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'evolution'\n",
    "experiment_path = 'exp_x'\n",
    "config_file = 'config_files_atleta/config4.txt'\n",
    "\n",
    "args = {\n",
    "    'experiment_path': experiment_path,\n",
    "    'config_file': config_file,\n",
    "    'data_path': 'atleta_axial_data',\n",
    "    'log_level': 'INFO',\n",
    "    'fitness_metric': 'best_accuracy',\n",
    "    'optimizer': 'adamw',\n",
    "    'data_augmentation': 'False',\n",
    "    \"early_stopping\": \"False\",\n",
    "    'dataset': 'atleta_axial',\n",
    "    'save_checkpoints_epochs': 10,\n",
    "}\n",
    "\n",
    "config = cfg.ConfigParameters(args, phase=phase)\n",
    "config.get_parameters()\n",
    "\n",
    "fn_dict=config.fn_dict\n",
    "\n",
    "# net_list = ['dconv_3_1_256','no_op', 'dconv_3_1_128', 'no_op','no_op','no_op',\n",
    "#             'dconv_3_1_64','dconv_3_1_64','no_op','dconv_3_1_256','dconv_3_1_256',\n",
    "#             'max_pool_2_2', 'dconv_3_1_128', 'no_op','no_op','no_op','no_op','no_op',\n",
    "#             'max_pool_2_2', 'dconv_3_1_128']\n",
    "# net_list = ['conv_3_1_256','no_op', 'cbam_7_7', 'no_op','no_op','no_op',\n",
    "#             'conv_3_1_64','conv_3_1_64','no_op','conv_3_1_256','conv_3_1_256',\n",
    "#             'max_pool_2_2', 'cbam_3_3', 'no_op','no_op','no_op','no_op','no_op',\n",
    "#             'max_pool_2_2', 'conv_3_1_128']\n",
    "\n",
    "#net_list = ['cbam_5_5', 'dconv_5_1_32', 'cbam_7_7', 'cbam_5_5', 'no_op', 'cbam_5_5', 'cbam_5_5', 'avg_pool_2_2', \n",
    "#            'cbam_7_7', 'no_op', 'no_op', 'no_op', 'no_op', 'conv_3_1_64', 'no_op', 'no_op', 'no_op', 'cbam_3_3', 'no_op', 'conv_1_1_32']\n",
    "\n",
    "# net_list = ['cbamconv_3_1_64', 'cbamconv_5_1_128', 'max_pool_2_2', 'resv1cbam_5_1_64', 'no_op', 'cbamconv_3_1_64', \n",
    "#             'resv1cbam_1_1_32', 'no_op', 'max_pool_2_2', 'conv_1_1_64', 'conv_1_1_128', 'cbamconv_5_1_64', 'avg_pool_2_2', \n",
    "#             'no_op', 'cbamconv_1_1_64', 'avg_pool_2_2', 'max_pool_2_2', 'no_op', 'cbamconv_5_1_32', 'avg_pool_2_2']\n",
    "\n",
    "\n",
    "# net_list = ['avg_pool_2_2','no_op', 'avg_pool_2_2','no_op','bv1_3_1_256','avg_pool_2_2',\n",
    "#             'bv1_3_1_128','no_op','max_pool_2_2', 'bv1p_3_1_128', 'bv1_3_1_256', 'no_op',\n",
    "#             'max_pool_2_2', 'max_pool_2_2','avg_pool_2_2','max_pool_2_2','bv1_3_1_64','avg_pool_2_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_list = ['no_op', 'bv1_3_1_128', 'max_pool_2_2','max_pool_2_2',\n",
    "            'no_op', 'bv1_3_1_128', 'avg_pool_2_2', 'avg_pool_2_2',\n",
    "            'no_op', 'bv1_3_1_256', 'no_op','bv1_3_1_128','bv1_3_1_128',\n",
    "            'bv1p_3_1_128', 'bv1_3_1_256', 'max_pool_2_2','max_pool_2_2',\n",
    "            'no_op', 'no_op', 'avg_pool_2_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config.train_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "desired_gpus = params[\"available_gpus\"]\n",
    "device_list = [f'cuda:{i}' for i in desired_gpus if i < torch.cuda.device_count()]\n",
    "device_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['device'] = device_list[0]\n",
    "params['num_workers'] = 4\n",
    "params['mixed_precision'] = True\n",
    "params['batch_size'] = 32\n",
    "params['eval_batch_size'] = 16\n",
    "params['max_epochs'] = 10\n",
    "params['epochs_to_eval'] = 9\n",
    "params['fn_dict'] = fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import estimate_model_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_memory = estimate_model_memory(net_list, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory: 1065.54931640625 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert the peak memory from bytes to MB\n",
    "peak_memory = peak_memory / 1024**2\n",
    "print(f\"Peak memory: {peak_memory} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cbam_key = any(key.startswith(\"cbam\") for key in fn_dict)\n",
    "has_cbam_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.train import fitness_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GenericDataLoader(params=params)\n",
    "train_loader, val_loader = data_loader.get_loader(pin_memory_device=device_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = fitness_calculation(id_num=\"job_0\", params=params, fn_dict=fn_dict, net_list=net_list, train_loader=train_loader, val_loader=val_loader,return_val=0 ,debug=True)\n",
    "#results_dict[\"best_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(results_dict, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Training With multiple architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a txt file containing the decoded nets\n",
    "import ast\n",
    "with open('config_files/decoded_nets_1.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    decoded_nets = [ast.literal_eval(line) for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 decoded nets\n",
    "for i in range(3):\n",
    "    print(f\"Net {i+1}: {decoded_nets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, net in enumerate(decoded_nets):\n",
    "    net_name = f\"Net_{i+1}\"\n",
    "    acc = fitness_calculation(id_num=net_name, params=params, fn_dict=fn_dict, net_list=net, debug=True)\n",
    "    acc_list[net_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_info_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_info_file(params['experiment_path'], acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(acc_list, orient='index', columns=['accuracy'])\n",
    "df = df.sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of the decoded nets\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(df.index, df['accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Net')\n",
    "plt.title('Accuracy of the decoded nets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qnas_config as cfg\n",
    "from util import check_files\n",
    "from cnn.input import GenericDataLoader\n",
    "from cnn.train_detailed import train_and_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'retrain'\n",
    "experiment_path = os.path.join(\"experiments\", \"exp13_adamw_repeat_2\")\n",
    "config_file = 'config_files/config11.txt'\n",
    "\n",
    "args = {\n",
    "    'experiment_path': experiment_path,\n",
    "    #'config_file': config_file,\n",
    "    'retrain_folder': 'retrain',\n",
    "    'data_path': 'cifar10_data',\n",
    "    'log_level': 'INFO',\n",
    "    'max_epochs': 300,\n",
    "    'epochs_to_eval': 10,\n",
    "    'batch_size': 256,\n",
    "    'eval_batch_size': 1000,\n",
    "    'limit_data': False,\n",
    "    'num_workers': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_files(args['experiment_path'])\n",
    "config = cfg.ConfigParameters(args, phase=phase)\n",
    "config.get_parameters()\n",
    "\n",
    "fn_dict=config.fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.load_evolved_data(experiment_path=experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config.train_spec\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolved_params = config.evolved_params\n",
    "evolved_params['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GenericDataLoader(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = data_loader.get_loader(pin_memory_device='cuda:0')\n",
    "test_loader = data_loader.get_loader(for_train=False, pin_memory_device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_multi = []\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    results = train_and_eval(params=params, fn_dict=fn_dict, net_list=evolved_params['net'], \n",
    "                             train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "    retrain_multi.append(results)\n",
    "    \n",
    "    params['experiment_path'] = os.path.join(experiment_path, f\"retrain_{i+1}\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the retraining into a txt file\n",
    "with open(os.path.join(experiment_path, 'retrain_results.txt'), 'w') as f:\n",
    "    for item in retrain_multi:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dict = train_and_eval(params=params, fn_dict=fn_dict, net_list=evolved_params['net'], \n",
    "#                               train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)\n",
    "# results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Multi-Objective Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mofitness(acc, params, inference_time, T_p=11.7e6, T_t=0.02):\n",
    "    \"\"\"\n",
    "    Calculate the fitness of an architecture a.\n",
    "\n",
    "    :param a: Architecture (this could be a placeholder, as the actual architecture details are abstracted in this function)\n",
    "    :param err: Classification error of the architecture a (0 <= err <= 1)\n",
    "    :param params: Number of parameters of the architecture a\n",
    "    :param inference_time: Inference time of the architecture a\n",
    "    :param T_p: Maximum allowable number of parameters (default is ResNet18)\n",
    "    :param T_t: Maximum allowable inference time (default is ResNet50 on CIFAR-10)\n",
    "    :return: Fitness value\n",
    "    \"\"\"\n",
    "    acc_in_range = 0 <= acc <= 1\n",
    "    acc = acc if acc_in_range else acc / 100.0\n",
    "        \n",
    "    # Determine weights based on parameters and inference time\n",
    "    if params <= T_p:\n",
    "        w_p = -0.01\n",
    "    else:\n",
    "        w_p = -1\n",
    "\n",
    "    if inference_time <= T_t:\n",
    "        w_t = -0.01\n",
    "    else:\n",
    "        w_t = -1\n",
    "\n",
    "    print(f\"params: {(params / T_p) ** w_p }\")\n",
    "    print(f\"inference_time: {(inference_time / T_t) ** w_t }\")\n",
    "    # Calculate the fitness function\n",
    "    fitness_value = (acc) * (params / T_p) ** w_p * (inference_time / T_t) ** w_t\n",
    "\n",
    "    fitness_value = fitness_value if acc_in_range else fitness_value * 100.0\n",
    "    \n",
    "    return fitness_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "architecture = \"example_architecture\"  # Placeholder\n",
    "classification_error = 0.9  # Classification error\n",
    "number_of_params = 1e6\n",
    "inference_time = 0.01  # Inference time in seconds\n",
    "\n",
    "fitness_value = mofitness(classification_error, number_of_params, inference_time)\n",
    "print(f\"Fitness value: {fitness_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import qnas_config as cfg\n",
    "from cnn.input import GenericDataLoader\n",
    "from cnn.fine_tune_cnn import load_trained_model, freeze_layers\n",
    "from util import load_log_params_evolution, load_retrain_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.atleta_dataloader import AtletaDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset_name': 'atleta_coronal',\n",
    "    #'config_file': config_file,\n",
    "    'retrain_folder': 'retrain',\n",
    "    'data_path': 'atleta_coronal_data',\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 16,\n",
    "    'data_augmentation': True,\n",
    "    'num_workers': 4,\n",
    "    'resize_size': (32, 32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoaders for the new dataset using AtletaDataLoader\n",
    "atleta_loader = AtletaDataLoader(\n",
    "    dataset_name=args['dataset_name'],\n",
    "    data_path=args['data_path'],\n",
    "    batch_size=args['batch_size'],\n",
    "    eval_batch_size=args['eval_batch_size'],\n",
    "    num_workers=args['num_workers'],\n",
    "    data_augmentation=args['data_augmentation'],\n",
    "    resize_size=args['resize_size'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = atleta_loader.get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apofisite_dir_esq', 'avulsao_dir_esq', 'normal_dir_esq']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atleta_loader.get_num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs, train_labels = next(iter(train_loader))\n",
    "train_imgs.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 32, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = 'experiments_organcmnist_base/exp1_repeat_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_log = load_log_params_evolution(experiment_path=experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_spec': {'available_gpus': [0, 1],\n",
       "  'batch_size': 128,\n",
       "  'data_augmentation': True,\n",
       "  'data_path': 'organcmnist_data',\n",
       "  'dataset': 'organcmnist',\n",
       "  'decay': 0.9,\n",
       "  'device': 'cuda:0',\n",
       "  'epochs_to_eval': 5,\n",
       "  'eval_batch_size': 128,\n",
       "  'experiment_path': 'experiments_organcmnist_base/exp1_repeat_1',\n",
       "  'fitness_metric': 'best_loss',\n",
       "  'learning_rate': 0.001,\n",
       "  'limit_data': False,\n",
       "  'limit_data_value': 10000,\n",
       "  'log_level': 'DEBUG',\n",
       "  'max_epochs': 50,\n",
       "  'max_inference_time': 3000,\n",
       "  'max_params': 15620000,\n",
       "  'mixed_precision': True,\n",
       "  'mo_metric_base': 'loss',\n",
       "  'momentum': 0.0,\n",
       "  'num_workers': 4,\n",
       "  'optimizer': 'AdamW',\n",
       "  'phase': 'evolution',\n",
       "  'save_checkpoints_epochs': 5,\n",
       "  'save_summary_epochs': 0.25,\n",
       "  'subtract_mean': True,\n",
       "  'threads': 8,\n",
       "  'weight_decay': 0.0001},\n",
       " 'QNAS_spec': {'crossover_frequency': 5,\n",
       "  'crossover_rate': 0.5,\n",
       "  'early_stopping': True,\n",
       "  'en_pop_crossover': False,\n",
       "  'fn_list': ['avg_pool_2_2',\n",
       "   'conv_1_1_64',\n",
       "   'conv_1_1_128',\n",
       "   'conv_1_1_256',\n",
       "   'conv_1_1_512',\n",
       "   'conv_3_1_64',\n",
       "   'conv_3_1_128',\n",
       "   'conv_3_1_256',\n",
       "   'conv_3_1_512',\n",
       "   'conv_5_1_64',\n",
       "   'conv_5_1_128',\n",
       "   'conv_5_1_256',\n",
       "   'conv_5_1_512',\n",
       "   'max_pool_2_2',\n",
       "   'no_op'],\n",
       "  'initial_probs': [0.16666666666666666,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.027777777777777776,\n",
       "   0.16666666666666666,\n",
       "   0.3333333333333333],\n",
       "  'max_generations': 150,\n",
       "  'max_num_nodes': 20,\n",
       "  'num_quantum_ind': 5,\n",
       "  'params_ranges': 'OrderedDict()',\n",
       "  'patience': 60,\n",
       "  'penalize_number': 3,\n",
       "  'pop_crossover_method': 'hux',\n",
       "  'pop_crossover_rate': 0.25,\n",
       "  'reducing_fns_list': ['avg_pool_2_2', 'max_pool_2_2'],\n",
       "  'repetition': 4,\n",
       "  'replace_method': 'best',\n",
       "  'save_data_freq': 10,\n",
       "  'update_quantum_gen': 5,\n",
       "  'update_quantum_rate': 0.1},\n",
       " 'fn_dict': {'avg_pool_2_2': {'function': 'AvgPooling',\n",
       "   'params': {'kernel': 2, 'strides': 2}},\n",
       "  'conv_1_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 128}},\n",
       "  'conv_1_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 256}},\n",
       "  'conv_1_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 512}},\n",
       "  'conv_1_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 64}},\n",
       "  'conv_3_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3, 'strides': 1, 'filters': 128}},\n",
       "  'conv_3_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3, 'strides': 1, 'filters': 256}},\n",
       "  'conv_3_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3, 'strides': 1, 'filters': 512}},\n",
       "  'conv_3_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3, 'strides': 1, 'filters': 64}},\n",
       "  'conv_5_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5, 'strides': 1, 'filters': 128}},\n",
       "  'conv_5_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5, 'strides': 1, 'filters': 256}},\n",
       "  'conv_5_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5, 'strides': 1, 'filters': 512}},\n",
       "  'conv_5_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5, 'strides': 1, 'filters': 64}},\n",
       "  'max_pool_2_2': {'function': 'MaxPooling',\n",
       "   'params': {'kernel': 2, 'strides': 2}},\n",
       "  'no_op': {'function': 'NoOp', 'params': {}}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_results = load_retrain_results(experiment_path=experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced the final FC layer successfully.\n"
     ]
    }
   ],
   "source": [
    "train_spec = params_log['train_spec']\n",
    "train_spec['fn_dict'] = params_log['fn_dict']\n",
    "train_spec['best_model_path'] = retrain_results['best_model_path']\n",
    "train_spec['num_classes_new'] = 3\n",
    "decode_net = retrain_results['net']\n",
    "model = load_trained_model(decode_net, train_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers(model, freeze_pattern=\"all_but_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'available_gpus': [0, 1],\n",
       " 'batch_size': 128,\n",
       " 'data_augmentation': True,\n",
       " 'data_path': 'organcmnist_data',\n",
       " 'dataset': 'organcmnist',\n",
       " 'decay': 0.9,\n",
       " 'device': 'cuda:0',\n",
       " 'epochs_to_eval': 5,\n",
       " 'eval_batch_size': 128,\n",
       " 'experiment_path': 'experiments_organcmnist_base/exp1_repeat_1',\n",
       " 'fitness_metric': 'best_loss',\n",
       " 'learning_rate': 0.001,\n",
       " 'limit_data': False,\n",
       " 'limit_data_value': 10000,\n",
       " 'log_level': 'DEBUG',\n",
       " 'max_epochs': 50,\n",
       " 'max_inference_time': 3000,\n",
       " 'max_params': 15620000,\n",
       " 'mixed_precision': True,\n",
       " 'mo_metric_base': 'loss',\n",
       " 'momentum': 0.0,\n",
       " 'num_workers': 4,\n",
       " 'optimizer': 'AdamW',\n",
       " 'phase': 'evolution',\n",
       " 'save_checkpoints_epochs': 5,\n",
       " 'save_summary_epochs': 0.25,\n",
       " 'subtract_mean': True,\n",
       " 'threads': 8,\n",
       " 'weight_decay': 0.0001,\n",
       " 'fn_dict': {'avg_pool_2_2': {'function': 'AvgPooling',\n",
       "   'params': {'kernel': 2, 'strides': 2}},\n",
       "  'conv_1_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 128}},\n",
       "  'conv_1_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 256}},\n",
       "  'conv_1_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1, 'strides': 1, 'filters': 512}},\n",
       "  'conv_1_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 1,\n",
       "    'strides': 1,\n",
       "    'filters': 64,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 512}},\n",
       "  'conv_3_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3,\n",
       "    'strides': 1,\n",
       "    'filters': 128,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 512}},\n",
       "  'conv_3_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3,\n",
       "    'strides': 1,\n",
       "    'filters': 256,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 256}},\n",
       "  'conv_3_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3,\n",
       "    'strides': 1,\n",
       "    'filters': 512,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 512}},\n",
       "  'conv_3_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 3,\n",
       "    'strides': 1,\n",
       "    'filters': 64,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 512}},\n",
       "  'conv_5_1_128': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5, 'strides': 1, 'filters': 128}},\n",
       "  'conv_5_1_256': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5,\n",
       "    'strides': 1,\n",
       "    'filters': 256,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 64}},\n",
       "  'conv_5_1_512': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5,\n",
       "    'strides': 1,\n",
       "    'filters': 512,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 64}},\n",
       "  'conv_5_1_64': {'function': 'ConvBlock',\n",
       "   'params': {'kernel': 5,\n",
       "    'strides': 1,\n",
       "    'filters': 64,\n",
       "    'mu': 0.9,\n",
       "    'epsilon': 2e-05,\n",
       "    'in_channels': 128}},\n",
       "  'max_pool_2_2': {'function': 'MaxPooling',\n",
       "   'params': {'kernel': 2, 'strides': 2}},\n",
       "  'no_op': {'function': 'NoOp', 'params': {}}},\n",
       " 'num_classes': 11,\n",
       " 'task': 'multi-class',\n",
       " 'input_shape': [128, 3, 28, 28],\n",
       " 'best_model_path': 'experiments_organcmnist_base/exp1_repeat_1/retrain_F13_1/best_model.pth'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cuda:0']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "desired_gpus = params[\"available_gpus\"]\n",
    "device_list = [f'cuda:{i}' for i in desired_gpus if i < torch.cuda.device_count()]\n",
    "device_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['device'] = device_list[0]\n",
    "params['num_workers'] = 4\n",
    "params['mixed_precision'] = True\n",
    "params['batch_size'] = 32\n",
    "params['eval_batch_size'] = 16\n",
    "params['max_epochs'] = 10\n",
    "params['epochs_to_eval'] = 9\n",
    "params['fn_dict'] = fn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.train import fitness_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GenericDataLoader(params=params)\n",
    "train_loader, val_loader = data_loader.get_loader(pin_memory_device=device_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
