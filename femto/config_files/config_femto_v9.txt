# Configuration file
# semelhante ai v0, mas o kernel de tamanho 2 foi substituido pelo kernel de 3
# gerações foi alterada 50, update_quantum_gen 3, update_quantum_rate 0.3
# max_update 0.15, max_epochs 30
# Semelhante ao v4, mas com preprocessing refatorado mais epocas, mais sensores e outras alteracoes
# IGual ao v5 mas com piecewise_lin_ref: 9400


QNAS:
    crossover_rate: 0.5
    max_generations: 50
    max_num_nodes: 6
    num_quantum_ind: 4
    penalize_number: 3
    repetition: 5
    replace_method: best
    update_quantum_gen: 3
    update_quantum_rate: 0.3
    save_data_freq: 10
    crossover_frequency: 5
    pop_crossover_rate: 0.25
    pop_crossover_method: hux # hux, uniform
    patience: 10
    early_stopping: False
    en_pop_crossover: False
    max_update: 0.15

    params_type:
        decay: float
        learning_rate: float
        weight_decay: int
        lstm_1: int
        lstm_2: int


    params_ranges:
        decay: 0.9
        learning_rate: 0.001
        weight_decay: 1.0e-4
        lstm_1: [4, 20]
        lstm_2: [4, 15]

    function_dict:  {
        'conv_1_1_4':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 4}, 'prob': 1.0/16.0},
        'conv_3_1_4':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 4}, 'prob': 1.0/16.0},
        'conv_1_1_3':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 3}, 'prob': 1.0/16.0},
        'conv_3_1_3':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 3}, 'prob': 1.0/16.0},
        'conv_1_1_9':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 9}, 'prob': 1.0/16.0},
        'conv_1_1_6':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 6}, 'prob': 1.0/16.0},
        'conv_1_1_7':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 7}, 'prob': 1.0/16.0},
        'conv_1_1_16':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 16}, 'prob': 1.0/16.0},
        'conv_3_1_5':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 5}, 'prob': 1.0/16.0},
        'conv_3_1_8':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 8}, 'prob': 1.0/16.0},
        'conv_3_1_9': {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 9}, 'prob': 1.0/16.0},
        'conv_3_1_16': {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 16}, 'prob': 1.0/16.0},
        'conv_3_1_10':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 10}, 'prob': 1.0/16.0},
        'conv_1_1_5':  {'function': 'Conv1DBlock', 'params': {'kernel': 1, 'strides': 1, 'filters': 5}, 'prob': 1.0/16.0},
        'conv_3_1_2':  {'function': 'Conv1DBlock', 'params': {'kernel': 3, 'strides': 1, 'filters': 2}, 'prob': 1.0/16.0},
        'no_op':        {'function': 'NoOp', 'params': {}, 'prob': 1.0/16.0}
    }

train:
    batch_size: 400
    eval_batch_size: 400
    max_epochs: 50
    epochs_to_eval: 49
    optimizer:
        name: RMSprop #RMSprop, Adam, AdamW, SGD
        params:
            lr: 0.001
            alpha: 0.9
            eps: 0.0000001
            momentum: 0.0
            centered: True
    fitness_metric: best_loss
    lr_scheduler: TFScheduler # for retrain
    lr_scheduler_train: TFScheduler
    criterion:
        name: MSELoss #CrossEntropyLoss, MSELoss, L1Loss, RMSELoss
        params:
            reduction: mean
    mo_metric_base: loss
    mixed_precision: False
    available_gpus: [0, 1] # gpu0, gpu1, gpu2 [0, 1, 2]
    device: cuda:0
    allow_duplicate_architectures: False

    # Dataset
    dataset: femto_multihead_train_v1
    file_extension: parquet
    exp_path_base: exp_femto
    exp: exp_v9
    log_level: INFO
    repeat: 3 # repetions for search
    dataloader_class: FemtoMultiHeadDataLoader

    data_augmentation: False
    subtract_mean: True
    limit_data: True
    limit_data_value: 10000
    num_workers: 0
    num_classes: 1
    task: regression
    dataset_type: multihead
    target_normalization:
        name: sklearn.preprocessing.MinMaxScaler
        path: femto_target_scaler_v1.save
        params: 

    # Tensorflow
    save_checkpoints_epochs: 5
    save_summary_epochs: 0.25
    threads: 16

    # penalization parameters - scalarized fitness
    max_params: 114090 # 1.9M
    max_inference_time: 1000 # us

    # Network structure Configuration
    network_config: default
    network_gap: False

    # Params for retrain:
    batch_size_retrain: 400
    eval_batch_size_retrain: 400
    max_epochs_retrain: 50
    num_repetitions_retrain: 5 # repetions for retrain

    # problem-specific parameters
    extra_params:
        shared_head_architecture: True
        num_sensors: 56
        dataset_test: femto_multihead_test_v1
        file_extension_test: parquet
        cols_non_sensor: ['bearing_id', 'RUL']
        cols_to_drop: ['file_name', 'sample_idx', 'rul_seconds', 'rul_files', 'elapsed_time', 'delta_seconds', 'dataset_type']
        sequence_length: 16
        stride: 1
        in_channels: 1
        val_split: 0.1
        piecewise_lin_ref: 9400
        scaler_name: femto_scaler_v1.save
        lstm_multiplier: 20
        validation_bearing_id: [12, 22, 32]
        type_validation: split_train_val_bearing # split_train_val_bearing, split_train_val_temporal, split_train_val_shuffle
        val_ratio: 0.2
        save_predictions_to_txt: True